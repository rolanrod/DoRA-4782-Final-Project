# cs-4782-final-project

## Ideas
https://www.reddit.com/r/MachineLearning/comments/16ij18f/d_the_ml_papers_that_rocked_our_world_20202023/

### NLP
- Small language model (probably still infeasible)
    - Language Models are Few-Shot Learners is safely a very infeasible paper to replicate
- Post-training LLaMA base model (I like this idea)
    - Grok says a 7B parameter model can be feasibly fine-tuned on a single GPU or trained from scratch on a small corpus with 2-4 GPUs. 

### Image
- ViT-Tiny, ViT-Small can be feasibly trained on smaller datasets on a single GPU
    - Fine-tuning tasks would be easier
- Vision Transformers Need Registers paper
- YOLOv4 --> feasible and trainable on a signle GPU
- Swin transformer --> hierarchical vision transformer using shifted windows


### Audio
- 